"""
Team 1 Quantum ‚Äî Benchmarks

Contract:
- Must not crash without CUDA (CPU fallback writes results).
- Must write to `benchmarks/results/latest.json` (relative to this file).
"""

from __future__ import annotations

import json
import os
import subprocess
import sys
import time
import threading
from typing import Any, Dict, List, Optional, Tuple
import platform

import torch

sys.path.append(os.path.join(os.path.dirname(__file__), "..", "src"))
from core import QuantumVacancyHunter


def _env_int(name: str, default: int, min_v: int, max_v: int) -> int:
    try:
        v = int(str(os.environ.get(name, "")).strip() or default)
    except Exception:
        v = default
    return max(min_v, min(max_v, v))


def _start_nvml_sampler(interval_s: float = 0.5) -> Tuple[Optional[threading.Event], Optional[threading.Thread], List[Dict[str, Any]]]:
    samples: List[Dict[str, Any]] = []
    try:
        import pynvml  # type: ignore
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    except Exception as e:
        samples.append({"ok": False, "error": f"nvml_unavailable: {e}"})
        return None, None, samples

    stop = threading.Event()

    def _loop() -> None:
        while not stop.is_set():
            try:
                u = pynvml.nvmlDeviceGetUtilizationRates(handle)
                mem = pynvml.nvmlDeviceGetMemoryInfo(handle)
                samples.append(
                    {
                        "ok": True,
                        "ts": time.time(),
                        "gpu_util": int(getattr(u, "gpu", 0)),
                        "mem_util": int(getattr(u, "memory", 0)),
                        "mem_used_mb": int(int(mem.used) // (1024 * 1024)),
                    }
                )
            except Exception as e:
                samples.append({"ok": False, "error": str(e)})
                break
            time.sleep(interval_s)

        try:
            pynvml.nvmlShutdown()
        except Exception:
            pass

    t = threading.Thread(target=_loop, daemon=True)
    t.start()
    return stop, t, samples


def _summarize_nvml(samples: List[Dict[str, Any]]) -> Dict[str, Any]:
    vals = [s.get("gpu_util") for s in samples if s.get("ok") and isinstance(s.get("gpu_util"), int)]
    mems = [s.get("mem_used_mb") for s in samples if s.get("ok") and isinstance(s.get("mem_used_mb"), int)]
    return {
        "samples": len(samples),
        "gpu_util_avg": float(sum(vals) / len(vals)) if vals else None,
        "gpu_util_max": int(max(vals)) if vals else None,
        "mem_used_mb_max": int(max(mems)) if mems else None,
        "nvml_ok": bool(vals),
    }


def _gpu_matmul_soak(seconds: int, n: int = 16384, dtype: torch.dtype = torch.bfloat16) -> Dict[str, Any]:
    # Sustained Tensor Core friendly workload: BF16 matmul loops for N seconds.
    device = torch.device("cuda")
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.set_float32_matmul_precision("high")

    a = torch.randn(n, n, device=device, dtype=dtype)
    b = torch.randn(n, n, device=device, dtype=dtype)

    # Warmup
    for _ in range(2):
        _ = a @ b
    torch.cuda.synchronize()

    t0 = time.perf_counter()
    iters = 0
    start_evt = torch.cuda.Event(enable_timing=True)
    end_evt = torch.cuda.Event(enable_timing=True)
    start_evt.record()
    while (time.perf_counter() - t0) < float(seconds):
        _ = a @ b
        iters += 1
    end_evt.record()
    torch.cuda.synchronize()

    elapsed_ms = float(start_evt.elapsed_time(end_evt))
    flops = 2.0 * float(n**3) * float(iters)
    tflops = (flops / max(1e-9, (elapsed_ms / 1000.0))) / 1e12
    return {
        "duration_s_target": int(seconds),
        "matrix_n": int(n),
        "dtype": str(dtype).replace("torch.", ""),
        "iters": int(iters),
        "elapsed_ms": elapsed_ms,
        "tflops_est": float(tflops),
    }


def _reserve_vram(target_gb: int, headroom_gb: int = 2) -> Tuple[Optional[torch.Tensor], Dict[str, Any]]:
    """
    Optionally reserve GPU VRAM so nvidia-smi shows 'heavy' memory usage.
    This is for demonstration / concurrency testing only; compute load is handled separately.
    """
    if target_gb <= 0 or not torch.cuda.is_available():
        return None, {"enabled": False}

    try:
        free_b, total_b = torch.cuda.mem_get_info()
        used_b = int(total_b - free_b)
        target_b = int(target_gb) * (1024**3)
        headroom_b = int(headroom_gb) * (1024**3)
        # Clamp: never try to allocate beyond what's free (minus headroom).
        can_alloc_b = max(0, int(free_b) - headroom_b)
        need_b = max(0, int(target_b) - int(used_b))
        alloc_b = min(need_b, can_alloc_b)
        if alloc_b <= 0:
            return None, {
                "enabled": True,
                "allocated_bytes": 0,
                "note": "Already at/above target or insufficient free memory",
                "used_bytes_before": used_b,
                "free_bytes_before": int(free_b),
                "total_bytes": int(total_b),
            }

        # Allocate bytes as uint8 elements.
        t = torch.empty(int(alloc_b), device="cuda", dtype=torch.uint8)
        torch.cuda.synchronize()
        free_a, total_a = torch.cuda.mem_get_info()
        return t, {
            "enabled": True,
            "target_gb": int(target_gb),
            "headroom_gb": int(headroom_gb),
            "allocated_bytes": int(alloc_b),
            "used_bytes_before": used_b,
            "free_bytes_before": int(free_b),
            "free_bytes_after": int(free_a),
            "total_bytes": int(total_a),
        }
    except Exception as e:
        return None, {"enabled": True, "error": str(e)}


def _results_path() -> str:
    out_dir = os.path.join(os.path.dirname(__file__), "results")
    os.makedirs(out_dir, exist_ok=True)
    return os.path.join(out_dir, "latest.json")


def _try_nvidia_smi() -> Dict[str, Any]:
    try:
        cmd = [
            "nvidia-smi",
            "--query-gpu=name,utilization.gpu,utilization.memory,memory.used,memory.total,power.draw",
            "--format=csv,noheader,nounits",
        ]
        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True).strip()
        return {"ok": True, "raw": out}
    except Exception as e:
        return {"ok": False, "error": str(e)}


def _probe_deps() -> Dict[str, Any]:
    deps: Dict[str, Any] = {
        "python": sys.version.split()[0],
        "platform": platform.platform(),
        "torch_version": getattr(torch, "__version__", None),
        "torch_cuda_version": getattr(getattr(torch, "version", None), "cuda", None),
        "cuda_available": bool(torch.cuda.is_available()),
    }
    if torch.cuda.is_available():
        try:
            deps["cuda_device"] = torch.cuda.get_device_name(0)
            deps["cuda_capability"] = list(torch.cuda.get_device_capability(0))
        except Exception as e:
            deps["cuda_device_error"] = str(e)

    # Optional cuQuantum/cuPy
    try:
        import cupy  # type: ignore
        deps["cupy"] = {"available": True, "version": getattr(cupy, "__version__", None)}
    except Exception as e:
        deps["cupy"] = {"available": False, "error": str(e)}

    try:
        import cuquantum  # type: ignore
        deps["cuquantum"] = {"available": True, "version": getattr(cuquantum, "__version__", None)}
    except Exception as e:
        deps["cuquantum"] = {"available": False, "error": str(e)}

    return deps


def run_benchmark() -> None:
    print("üöÄ STARTING BENCHMARK ‚Äî Team 1 Quantum (QuantumVacancyHunter)")

    out_path = _results_path()

    if not torch.cuda.is_available():
        payload = {
            "status": "cuda_unavailable",
            "device": "cpu",
            "error": "No CUDA device",
            "deps": _probe_deps(),
            "nvidia_smi": _try_nvidia_smi(),
        }
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(payload, f, indent=2)
        print(f"‚ö†Ô∏è CUDA unavailable. Wrote {out_path}")
        return

    dev_name = torch.cuda.get_device_name(0)
    bench_seconds = _env_int("BENCH_SECONDS", 60, 4, 180)
    reserve_gb = _env_int("BENCH_RESERVE_GB", 0, 0, 78)
    results: Dict[str, Any] = {
        "status": "success",
        "device": dev_name,
        "deps": _probe_deps(),
        "nvidia_smi_before": _try_nvidia_smi(),
        "benchmarks": {},
        "bench_seconds": bench_seconds,
        "bench_reserve_gb": reserve_gb,
    }

    # Heavy path: [B,N] @ [N,N] repeated; Tensor Cores kick in for FP16.
    cases = [
        {"lattice_size": 4096, "batch": 512, "steps": 10, "use_fp16": True},
        {"lattice_size": 8192, "batch": 256, "steps": 8, "use_fp16": True},
    ]

    # Optional: reserve VRAM up to BENCH_RESERVE_GB (for "34GB heavy" demonstrations).
    reserve_tensor, reserve_info = _reserve_vram(reserve_gb, headroom_gb=2)
    results["benchmarks"]["vram_reserve"] = reserve_info

    # Start NVML sampling so we can PROVE avg/max SM util even when kernels are short.
    stop_nvml, nvml_thread, nvml_samples = _start_nvml_sampler(interval_s=0.5)

    for c in cases:
        key = f"lattice={c['lattice_size']}_batch={c['batch']}_steps={c['steps']}_fp16={c['use_fp16']}"
        print(f"Testing {key} ...")
        hunter = QuantumVacancyHunter(lattice_size=c["lattice_size"], use_fp16=c["use_fp16"])

        # Warmup (small)
        _ = hunter.find_stable_vacancies(batch_size=min(c["batch"], 64), steps=2)

        torch.cuda.synchronize()
        # Keep total runtime ~bench_seconds: spend a small slice per case, then do a matmul soak.
        per_case_s = max(2, int(bench_seconds * 0.2))
        t0 = time.perf_counter()
        iters = 0
        last_m = None
        while (time.perf_counter() - t0) < float(per_case_s):
            last_m = hunter.find_stable_vacancies(batch_size=c["batch"], steps=c["steps"])
            iters += 1
        torch.cuda.synchronize()
        wall_s = time.perf_counter() - t0
        m = last_m  # type: ignore[assignment]

        # FLOPs per step: (B,N)@(N,N) -> 2*B*N*N
        B = float(c["batch"])
        N = float(c["lattice_size"])
        flops = 2.0 * B * (N**2) * float(c["steps"]) * float(max(1, iters))
        tflops = (flops / max(1e-9, wall_s)) / 1e12

        results["benchmarks"][key] = {
            "wall_time_s": round(wall_s, 6),
            "tflops_est": float(tflops),
            "iters": int(iters),
            "metrics": {
                "compute_time_ms": float(getattr(m, "compute_time_ms", 0.0)),
                "fidelity": float(getattr(m, "fidelity", 0.0)),
                "active_qubits": int(getattr(m, "active_qubits", 0)),
                "details": dict(getattr(m, "details", {})),
            },
        }

        print(f"  -> wall={wall_s:.3f}s | iters={iters} | TFLOPS(est)={tflops:.2f}")

    # Hard proof: sustained Tensor Core matmul for the remaining time budget.
    remaining_s = max(4, int(bench_seconds - (per_case_s * len(cases))))
    results["benchmarks"]["gpu_soak_matmul"] = _gpu_matmul_soak(seconds=remaining_s, n=16384, dtype=torch.bfloat16)

    if stop_nvml is not None:
        stop_nvml.set()
    if nvml_thread is not None:
        nvml_thread.join(timeout=2.0)
    results["nvml"] = _summarize_nvml(nvml_samples)

    results["nvidia_smi_after"] = _try_nvidia_smi()

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2)

    print(f"‚úÖ BENCHMARK COMPLETE. Wrote {out_path}")

    # Keep reserved VRAM alive until after writing results.
    _ = reserve_tensor


if __name__ == "__main__":
    run_benchmark()
